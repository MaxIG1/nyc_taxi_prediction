{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\raw\\\\*.csv\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(\n",
    "    modified_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Combining all month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date/Time</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Base</th>\n",
       "      <th>Date</th>\n",
       "      <th>Date_1</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/1/2014 0:00:00</td>\n",
       "      <td>40.7637</td>\n",
       "      <td>-73.9600</td>\n",
       "      <td>B02598</td>\n",
       "      <td>2014-04-01 00:00:00</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/1/2014 0:00:00</td>\n",
       "      <td>40.7188</td>\n",
       "      <td>-73.9863</td>\n",
       "      <td>B02598</td>\n",
       "      <td>2014-04-01 00:00:00</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/1/2014 0:00:00</td>\n",
       "      <td>40.7215</td>\n",
       "      <td>-73.9952</td>\n",
       "      <td>B02682</td>\n",
       "      <td>2014-04-01 00:00:00</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/1/2014 0:01:00</td>\n",
       "      <td>40.7355</td>\n",
       "      <td>-73.9966</td>\n",
       "      <td>B02617</td>\n",
       "      <td>2014-04-01 00:01:00</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/1/2014 0:02:00</td>\n",
       "      <td>40.7184</td>\n",
       "      <td>-73.9601</td>\n",
       "      <td>B02682</td>\n",
       "      <td>2014-04-01 00:02:00</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date/Time      Lat      Lon    Base                Date      Date_1  \\\n",
       "0  4/1/2014 0:00:00  40.7637 -73.9600  B02598 2014-04-01 00:00:00  2014-04-01   \n",
       "1  4/1/2014 0:00:00  40.7188 -73.9863  B02598 2014-04-01 00:00:00  2014-04-01   \n",
       "2  4/1/2014 0:00:00  40.7215 -73.9952  B02682 2014-04-01 00:00:00  2014-04-01   \n",
       "3  4/1/2014 0:01:00  40.7355 -73.9966  B02617 2014-04-01 00:01:00  2014-04-01   \n",
       "4  4/1/2014 0:02:00  40.7184 -73.9601  B02682 2014-04-01 00:02:00  2014-04-01   \n",
       "\n",
       "   Hour  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "# Read each CSV file into a DataFrame and add it to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame vertically\n",
    "combined_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# sort by Date/Time and rest index\n",
    "combined_df.sort_values(by=\"Date/Time\", inplace=True)\n",
    "combined_df.reset_index(inplace=True)\n",
    "combined_df.drop(columns='index', inplace=True)\n",
    "\n",
    "# transform to pd_date\n",
    "combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date/Time\"])\n",
    "combined_df[\"Date_1\"] = combined_df[\"Date\"].dt.date\n",
    "combined_df[\"Hour\"] = combined_df[\"Date\"].dt.hour\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"combined_data_all_month.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "combined_df.to_csv(modified_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing the april data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data-apr14.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "\n",
    "replacement_string = \"data\\\\raw\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string + file_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(modified_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date/Time\"])\n",
    "df[\"index\"] = df.index\n",
    "df[\"Date_1\"] = df[\"Date\"].dt.date\n",
    "df[\"Hour\"] = df[\"Date\"].dt.hour\n",
    "\n",
    "\n",
    "df_grouped_april = (\n",
    "    df.groupby([\"Date_1\", \"Hour\"], as_index=False)\n",
    "    .agg(\n",
    "        {\n",
    "            \"index\": \"count\",  # Replace 'column1' with the name of the column you want to sum\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df_grouped_april.drop(columns=\"level_0\", inplace=True)\n",
    "df_grouped_april.columns = [\"Date\", \"Hour\", \"Count\"]\n",
    "df_grouped_april[\"Date\"] = pd.to_datetime(df_grouped_april[\"Date\"])\n",
    "\n",
    "df_grouped_april[\"CombinedDateTime\"] = df_grouped_april[\"Date\"] + pd.to_timedelta(\n",
    "    df_grouped_april[\"Hour\"], unit=\"h\"\n",
    ")\n",
    "df_grouped_april.set_index(\"CombinedDateTime\", inplace=True)\n",
    "df_grouped_april.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"april_data_processed.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "# Save the DataFrame to the CSV file in the specified folder\n",
    "df_grouped_april.to_csv(modified_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## April Data sliced into weekdays and weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max_G\\AppData\\Local\\Temp\\ipykernel_24716\\1331811347.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_april_week[\"diff_Count1\"] = df_april_week[\"Count\"].diff(periods=1)\n",
      "C:\\Users\\Max_G\\AppData\\Local\\Temp\\ipykernel_24716\\1331811347.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_april_week[\"diff_Count24+1\"] = df_april_week[\"diff_Count1\"].diff(periods=24)\n",
      "C:\\Users\\Max_G\\AppData\\Local\\Temp\\ipykernel_24716\\1331811347.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_april_week[\"diff_Count24\"] = df_april_week[\"Count\"].diff(periods=24)\n",
      "C:\\Users\\Max_G\\AppData\\Local\\Temp\\ipykernel_24716\\1331811347.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_april_end[\"diff_Count1\"] = df_april_end[\"Count\"].diff(periods=1)\n",
      "C:\\Users\\Max_G\\AppData\\Local\\Temp\\ipykernel_24716\\1331811347.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_april_end[\"diff_Count24+1\"] = df_april_end[\"diff_Count1\"].diff(periods=24)\n",
      "C:\\Users\\Max_G\\AppData\\Local\\Temp\\ipykernel_24716\\1331811347.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_april_end[\"diff_Count24\"] = df_april_end[\"Count\"].diff(periods=24)\n"
     ]
    }
   ],
   "source": [
    "# Creating weekdays for improving the\n",
    "df_grouped_april[\"Weekday\"] = df_grouped_april[\"Date\"].dt.weekday\n",
    "\n",
    "weekday_names = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "df_grouped_april[\"Weekday\"] = df_grouped_april[\"Weekday\"].map(lambda x: weekday_names[x])\n",
    "\n",
    "\n",
    "# lets split up the date and see if we can achiev better results.\n",
    "\n",
    "weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "weekend = [\"Saturday\", \"Sunday\"]\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_april_week = df_grouped_april.query(\"Weekday in @weekdays\")\n",
    "df_april_end = df_grouped_april.query(\"Weekday in @weekend\")\n",
    "\n",
    "# Differencing\n",
    "df_april_week[\"diff_Count1\"] = df_april_week[\"Count\"].diff(periods=1)\n",
    "df_april_week[\"diff_Count24+1\"] = df_april_week[\"diff_Count1\"].diff(periods=24)\n",
    "df_april_week[\"diff_Count24\"] = df_april_week[\"Count\"].diff(periods=24)\n",
    "\n",
    "df_april_end[\"diff_Count1\"] = df_april_end[\"Count\"].diff(periods=1)\n",
    "df_april_end[\"diff_Count24+1\"] = df_april_end[\"diff_Count1\"].diff(periods=24)\n",
    "df_april_end[\"diff_Count24\"] = df_april_end[\"Count\"].diff(periods=24)\n",
    "\n",
    "\n",
    "# Save the DataFrame to the CSV file in the specified folder\n",
    "file_name = \"df_april_week.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "df_april_week.to_csv(modified_path, index=True)\n",
    "\n",
    "# Save the DataFrame to the CSV file in the specified folder\n",
    "file_name = \"df_april_end.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "df_april_end.to_csv(modified_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing the may data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data-may14.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "\n",
    "replacement_string = \"data\\\\raw\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "df_may = pd.read_csv(modified_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_may = pd.read_csv(\n",
    "    r\"C:\\Users\\Max_G\\OneDrive\\IUBH\\5_Semester\\Model_Engeneering\\Public_Transport_Forecasting\\data\\raw\\data-may14.csv\"\n",
    ")\n",
    "\n",
    "df_may[\"Date\"] = pd.to_datetime(df_may[\"Date/Time\"])\n",
    "df_may[\"index\"] = df_may.index\n",
    "df_may[\"Date_1\"] = df_may[\"Date\"].dt.date\n",
    "df_may[\"Hour\"] = df_may[\"Date\"].dt.hour\n",
    "\n",
    "df_grouped_may = (\n",
    "    df_may.groupby([\"Date_1\", \"Hour\"], as_index=False)\n",
    "    .agg(\n",
    "        {\n",
    "            \"index\": \"count\",  # Replace 'column1' with the name of the column you want to sum\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "df_grouped_may.drop(columns=\"level_0\", inplace=True)\n",
    "df_grouped_may.columns = [\"Date\", \"Hour\", \"Count\"]\n",
    "df_grouped_may[\"Date\"] = pd.to_datetime(df_grouped_may[\"Date\"])\n",
    "\n",
    "df_grouped_may[\"CombinedDateTime\"] = df_grouped_may[\"Date\"] + pd.to_timedelta(\n",
    "    df_grouped_may[\"Hour\"], unit=\"h\"\n",
    ")\n",
    "df_grouped_may.set_index(\"CombinedDateTime\", inplace=True)\n",
    "df_grouped_may.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"may_data_processed.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "# Save the DataFrame to the CSV file in the specified folder\n",
    "df_grouped_may.to_csv(modified_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May Data sliced into weekdays and weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating weekdays for improving the\n",
    "df_grouped_may[\"Weekday\"] = df_grouped_may[\"Date\"].dt.weekday\n",
    "\n",
    "weekday_names = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "df_grouped_may[\"Weekday\"] = df_grouped_may[\"Weekday\"].map(lambda x: weekday_names[x])\n",
    "\n",
    "\n",
    "# lets split up the date and see if we can achiev better results.\n",
    "\n",
    "weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "weekend = [\"Saturday\", \"Sunday\"]\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_may_week = df_grouped_may.query(\"Weekday in @weekdays\")\n",
    "df_may_end = df_grouped_may.query(\"Weekday in @weekend\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the DataFrame to the CSV file in the specified folder\n",
    "file_name = \"df_may_week.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "df_may_week.to_csv(modified_path, index=True)\n",
    "\n",
    "\n",
    "# Save the DataFrame to the CSV file in the specified folder\n",
    "file_name = \"df_may_end.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "df_may_end.to_csv(modified_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preparing the clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"combined_month_clustered_6.csv\"\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string + file_name \n",
    "\n",
    "df = pd.read_csv(modified_path, index_col=[0])\n",
    "df.Date = pd.to_datetime(df['Date'])\n",
    "df[\"index\"] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "weekend = [ 'Saturday', 'Sunday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting in cluster, month, weekday/week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_grouped = (\n",
    "            df.groupby([\"Date_1\", \"Hour\"], as_index=False)\n",
    "            .agg({\"index\": \"count\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "df_all_data_grouped.drop(columns=\"level_0\", inplace=True)\n",
    "df_all_data_grouped.columns = [\"Date\", \"Hour\", \"Count\"]\n",
    "df_all_data_grouped[\"Date\"] = pd.to_datetime(df_all_data_grouped[\"Date\"])\n",
    "df_all_data_grouped[\"CombinedDateTime\"] = (\n",
    "            df_all_data_grouped[\"Date\"]\n",
    "            + pd.to_timedelta(df_all_data_grouped[\"Hour\"], unit=\"h\")\n",
    "        )\n",
    "df_all_data_grouped.set_index(\"CombinedDateTime\", inplace=True)\n",
    "df_all_data_grouped.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_grouped[\"Weekday\"] = df_all_data_grouped[\"Date\"].dt.weekday\n",
    "df_all_data_grouped[\"Weekday\"] = df_all_data_grouped[\"Weekday\"].map(lambda x: weekday_names[x])\n",
    "\n",
    "# Split the data into weekday and weekend DataFrames\n",
    "df_all_data_grouped_week = df_all_data_grouped.query(\"Weekday in @weekdays\")\n",
    "# current_df_week.reset_index(drop=True, inplace=True)\n",
    "df_all_data_grouped_end = df_all_data_grouped.query(\"Weekday in @weekend\")\n",
    "# current_df_end.reset_index(drop=True, inplace=True)\n",
    "\n",
    " # Define the file names based on the label and month\n",
    "weekday_file_name = f\"df_all_data_grouped_weekday.csv\"\n",
    "weekend_file_name = f\"df_all_data_grouped_weekend.csv\"\n",
    "grouped_file_name = f\"df_all_data_grouped.csv\"\n",
    "\n",
    "\n",
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\Clustered Data\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path_weekday = modified_path + \"\\\\\" + replacement_string + weekday_file_name\n",
    "modified_path_weekend = modified_path + \"\\\\\" + replacement_string + weekend_file_name\n",
    "modified_path_grouped = modified_path + \"\\\\\" + replacement_string + grouped_file_name \n",
    "\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "df_all_data_grouped_end.to_csv(modified_path_weekend, index=True)\n",
    "df_all_data_grouped_week.to_csv(modified_path_weekday, index=True)\n",
    "df_all_data_grouped.to_csv(modified_path_grouped, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = df['labels'].unique()\n",
    "\n",
    "# Create a dictionary to store data frames for each label and month\n",
    "neighborhood_dict = {}\n",
    "\n",
    "# Define the list of months\n",
    "month_list = [4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Iterate through labels and months\n",
    "for label in label_list:\n",
    "    neighborhood_dict[label] = {}  \n",
    "    for month in month_list:\n",
    "        # Filter the DataFrame for the current label and month\n",
    "        df_filtered = df[(df['labels'] == label) & (df['Date'].dt.month == month)]\n",
    "        neighborhood_dict[label][month] = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dict = {}\n",
    "\n",
    "for label in neighborhood_dict:\n",
    "    nested_dict[label] = {}  # Create an inner dictionary for each label\n",
    "    for month in neighborhood_dict[label]:\n",
    "        # Extract the DataFrame for the current label and month\n",
    "        current_df = neighborhood_dict[label][month]\n",
    "        \n",
    "        # Perform your DataFrame manipulations here\n",
    "        current_df_grouped = (\n",
    "            current_df.groupby([\"Date_1\", \"Hour\"], as_index=False)\n",
    "            .agg({\"index\": \"count\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        current_df_grouped.drop(columns=\"level_0\", inplace=True)\n",
    "        current_df_grouped.columns = [\"Date\", \"Hour\", \"Count\"]\n",
    "        current_df_grouped[\"Date\"] = pd.to_datetime(current_df_grouped[\"Date\"])\n",
    "        current_df_grouped[\"CombinedDateTime\"] = (\n",
    "            current_df_grouped[\"Date\"]\n",
    "            + pd.to_timedelta(current_df_grouped[\"Hour\"], unit=\"h\")\n",
    "        )\n",
    "        current_df_grouped.set_index(\"CombinedDateTime\", inplace=True)\n",
    "        current_df_grouped.index.name = None\n",
    "        \n",
    "        # Add the current DataFrame to the inner dictionary\n",
    "        nested_dict[label][month] = current_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_nested_dict = {}\n",
    "\n",
    "# Loop through the original nested_dict\n",
    "for label in nested_dict:\n",
    "    transformed_nested_dict[label] = {}  # Create an inner dictionary for each label\n",
    "    for month in nested_dict[label]:\n",
    "        # Extract the DataFrame for the current label, month, and hour\n",
    "        current_df = nested_dict[label][month]\n",
    "\n",
    "        # Apply the transformations\n",
    "        current_df[\"Weekday\"] = current_df[\"Date\"].dt.weekday\n",
    "        current_df[\"Weekday\"] = current_df[\"Weekday\"].map(lambda x: weekday_names[x])\n",
    "\n",
    "        # Split the data into weekday and weekend DataFrames\n",
    "        current_df_week = current_df.query(\"Weekday in @weekdays\")\n",
    "        # current_df_week.reset_index(drop=True, inplace=True)\n",
    "        current_df_end = current_df.query(\"Weekday in @weekend\")\n",
    "        # current_df_end.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Store the transformed DataFrames in the inner dictionary\n",
    "        transformed_nested_dict[label][month] = {\n",
    "            \"Weekday\": current_df_week,\n",
    "            \"Weekend\": current_df_end,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\Clustered Data\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in transformed_nested_dict:\n",
    "    for month in transformed_nested_dict[label]:\n",
    "        # Extract the transformed DataFrames for weekday and weekend\n",
    "        weekday_df = transformed_nested_dict[label][month][\"Weekday\"]\n",
    "        weekend_df = transformed_nested_dict[label][month][\"Weekend\"]\n",
    "\n",
    "        # Define the file names based on the label and month\n",
    "        weekday_file_name = f\"df_{label}_month_{month}_weekday.csv\"\n",
    "        weekend_file_name = f\"df_{label}_month_{month}_weekend.csv\"\n",
    "\n",
    "        # Create the full paths for saving\n",
    "        weekday_full_path = modified_path + weekday_file_name\n",
    "        weekend_full_path = modified_path + weekend_file_name\n",
    "\n",
    "        # Save the DataFrames to CSV files\n",
    "        weekday_df.to_csv(weekday_full_path, index=True)\n",
    "        weekend_df.to_csv(weekend_full_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting in Cluster and weekday/week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = df['labels'].unique()\n",
    "\n",
    "# Create a dictionary to store data frames for each label and month\n",
    "neighborhood_dict = {}\n",
    "\n",
    "# Iterate through labels and months\n",
    "for label in label_list:\n",
    "    # Filter the DataFrame for the current label and month\n",
    "    df_filtered = df[(df['labels'] == label)]\n",
    "    neighborhood_dict[label]= df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dict_grouped = {}\n",
    "\n",
    "for label in neighborhood_dict:\n",
    "    \n",
    "    current_df = neighborhood_dict[label]\n",
    "        \n",
    "        # Perform your DataFrame manipulations here\n",
    "    current_df_grouped = (\n",
    "        current_df.groupby([\"Date_1\", \"Hour\"], as_index=False)\n",
    "        .agg({\"index\": \"count\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    current_df_grouped.drop(columns=\"level_0\", inplace=True)\n",
    "    current_df_grouped.columns = [\"Date\", \"Hour\", \"Count\"]\n",
    "    current_df_grouped[\"Date\"] = pd.to_datetime(current_df_grouped[\"Date\"])\n",
    "    current_df_grouped[\"CombinedDateTime\"] = (\n",
    "        current_df_grouped[\"Date\"]\n",
    "        + pd.to_timedelta(current_df_grouped[\"Hour\"], unit=\"h\")\n",
    "    )\n",
    "    current_df_grouped.set_index(\"CombinedDateTime\", inplace=True)\n",
    "    current_df_grouped.index.name = None\n",
    "\n",
    "    \n",
    "    # Add the current DataFrame to the inner dictionary\n",
    "    neighborhood_dict_grouped[label] = current_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "weekend = [ 'Saturday', 'Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are missing values after aggregating, need to fill in 0.\n",
    "for label in neighborhood_dict_grouped:\n",
    "    \n",
    "    current_df_grouped = neighborhood_dict_grouped[label]\n",
    "\n",
    "    start_date = neighborhood_dict_grouped[0].index[0] \n",
    "    end_date = neighborhood_dict_grouped[0].index[-1] \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "    current_df_grouped = current_df_grouped.reindex(date_range)\n",
    "    current_df_grouped['Count'].fillna(0, inplace=True)\n",
    "    current_df_grouped['Date'] = current_df_grouped.index.date\n",
    "    current_df_grouped['Hour'] = current_df_grouped.index.hour\n",
    "\n",
    "    neighborhood_dict_grouped[label] = current_df_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dict_grouped_diff_week = {}\n",
    "\n",
    "# Loop through the original nested_dict\n",
    "for label in neighborhood_dict_grouped:\n",
    "    # Extract the DataFrame for the current label, month, and hour\n",
    "    current_df = neighborhood_dict_grouped[label]\n",
    "    \n",
    "    current_df['Date'] = pd.to_datetime(current_df_grouped[\"Date\"])\n",
    "        # Apply the transformations\n",
    "    current_df[\"Weekday\"] = current_df[\"Date\"].dt.weekday\n",
    "    current_df[\"Weekday\"] = current_df[\"Weekday\"].map(lambda x: weekday_names[x])\n",
    "\n",
    "    # Split the data into weekday and weekend DataFrames\n",
    "    current_df_week = current_df.query(\"Weekday in @weekdays\")\n",
    "    current_df_end = current_df.query(\"Weekday in @weekend\")\n",
    "    \n",
    "\n",
    "    # Store the transformed DataFrames in the inner dictionary\n",
    "    neighborhood_dict_grouped_diff_week[label] = {\n",
    "        \"Weekday\": current_df_week,\n",
    "        \"Weekend\": current_df_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\Cluster_Data_All_Month\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in  neighborhood_dict_grouped_diff_week:\n",
    "    # Extract the transformed DataFrames for weekday and weekend\n",
    "    weekday_df = neighborhood_dict_grouped_diff_week[label][\"Weekday\"]\n",
    "    weekend_df = neighborhood_dict_grouped_diff_week[label][\"Weekend\"]\n",
    "\n",
    "    # Define the file names based on the label and month\n",
    "    weekday_file_name = f\"df_{label}_weekday.csv\"\n",
    "    weekend_file_name = f\"df_{label}_weekend.csv\"\n",
    "\n",
    "    # Create the full paths for saving\n",
    "    weekday_full_path = modified_path + weekday_file_name\n",
    "    weekend_full_path = modified_path + weekend_file_name\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    weekday_df.to_csv(weekday_full_path, index=True)\n",
    "    weekend_df.to_csv(weekend_full_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data in clusters and then grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = df['labels'].unique()\n",
    "\n",
    "# Create a dictionary to store data frames for each label and month\n",
    "neighborhood_dict = {}\n",
    "\n",
    "# Iterate through labels and months\n",
    "for label in label_list:\n",
    "    # Filter the DataFrame for the current label and month\n",
    "    df_filtered = df[(df['labels'] == label)]\n",
    "    neighborhood_dict[label]= df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dict_grouped = {}\n",
    "\n",
    "for label in neighborhood_dict:\n",
    "    \n",
    "    current_df = neighborhood_dict[label]\n",
    "        \n",
    "        # Perform your DataFrame manipulations here\n",
    "    current_df_grouped = (\n",
    "        current_df.groupby([\"Date_1\", \"Hour\"], as_index=False)\n",
    "        .agg({\"index\": \"count\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    current_df_grouped.drop(columns=\"level_0\", inplace=True)\n",
    "    current_df_grouped.columns = [\"Date\", \"Hour\", \"Count\"]\n",
    "    current_df_grouped[\"Date\"] = pd.to_datetime(current_df_grouped[\"Date\"])\n",
    "    current_df_grouped[\"CombinedDateTime\"] = (\n",
    "        current_df_grouped[\"Date\"]\n",
    "        + pd.to_timedelta(current_df_grouped[\"Hour\"], unit=\"h\")\n",
    "    )\n",
    "    current_df_grouped.set_index(\"CombinedDateTime\", inplace=True)\n",
    "    current_df_grouped.index.name = None\n",
    "\n",
    "    \n",
    "    # Add the current DataFrame to the inner dictionary\n",
    "    neighborhood_dict_grouped[label] = current_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are missing values after aggregating, need to fill in 0.\n",
    "for label in neighborhood_dict_grouped:\n",
    "    \n",
    "    current_df_grouped = neighborhood_dict_grouped[label]\n",
    "\n",
    "    start_date = neighborhood_dict_grouped[0].index[0] \n",
    "    end_date = neighborhood_dict_grouped[0].index[-1] \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "    current_df_grouped = current_df_grouped.reindex(date_range)\n",
    "    current_df_grouped['Count'].fillna(0, inplace=True)\n",
    "    current_df_grouped['Date'] = current_df_grouped.index.date\n",
    "    current_df_grouped['Hour'] = current_df_grouped.index.hour\n",
    "\n",
    "    neighborhood_dict_grouped[label] = current_df_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dict_grouped_diff_week = {}\n",
    "\n",
    "# Loop through the original nested_dict\n",
    "for label in neighborhood_dict_grouped:\n",
    "    # Extract the DataFrame for the current label, month, and hour\n",
    "    current_df = neighborhood_dict_grouped[label]\n",
    "    \n",
    "    current_df['Date'] = pd.to_datetime(current_df_grouped[\"Date\"])\n",
    "        # Apply the transformations\n",
    "    current_df[\"Weekday\"] = current_df[\"Date\"].dt.weekday\n",
    "    current_df[\"Month\"] = current_df[\"Date\"].dt.month\n",
    "    current_df['IsWeekend'] = current_df['Weekday'].apply(lambda day: 1 if day in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    neighborhood_dict_grouped[label] = current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in neighborhood_dict_grouped:\n",
    "    neighborhood_dict_grouped[label] =  neighborhood_dict_grouped[label].rename(columns={'Count': f'Count_{label}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements in the list are True\n"
     ]
    }
   ],
   "source": [
    "# checking if all have the same lenght\n",
    "\n",
    "true_list = []\n",
    "for label in neighborhood_dict_grouped:\n",
    "    if len(neighborhood_dict_grouped[label]) == 4391:\n",
    "        true_list.append(True)\n",
    "    else:\n",
    "        true_list.append(False)\n",
    "\n",
    "if all(true_list):\n",
    "    print(\"All elements in the list are True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in neighborhood_dict_grouped:\n",
    "    if label > 0:\n",
    "        neighborhood_dict_grouped[0][f'Count_{label}'] = neighborhood_dict_grouped[label][f'Count_{label}']\n",
    "\n",
    "df_all_counts = neighborhood_dict_grouped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"1. Data_Preparation.ipynb\")\n",
    "notebook_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "replacement_string = \"data\\\\interim\\\\Cluster_Data_All_Month\\\\\"\n",
    "index = notebook_directory.find(\"\\\\notebooks\")\n",
    "modified_path = notebook_directory[:index]\n",
    "\n",
    "modified_path += \"\\\\\" + replacement_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"df_all_counts_grouped.csv\"\n",
    "\n",
    "# Create the full paths for saving\n",
    "full_path = modified_path + file_name\n",
    "\n",
    "df_all_counts.to_csv(full_path, index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
